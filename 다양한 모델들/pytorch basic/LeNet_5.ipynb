{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeNet-5.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZcD6nTpkm6w"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content1/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import utils\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "cl0ptaJak8w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, x_data, y_data, transform=None):\n",
        "        self.x_data = x_data\n",
        "        self.y_data = y_data\n",
        "        self.transform = transform\n",
        "        self.len = len(y_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.x_data[index], self.y_data[index]\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        \n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "class ToTensor:\n",
        "    def __call__(self, sample):\n",
        "        inputs, labels = sample\n",
        "        inputs = torch.FloatTensor(inputs)\n",
        "        inputs = inputs.permute(2,0,1)\n",
        "        return inputs, torch.LongTensor(labels)\n",
        "\n",
        "class Resize:\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int,tuple))\n",
        "        self.ouput_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        inputs, labels = sample\n",
        "\n",
        "        h, w = inputs.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w :\n",
        "                new_h, new_w = self.output_size*h/w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size*w/h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "\n",
        "        new_inputs = tf.resize(inputs, (new_h, new_w))\n",
        "        \n",
        "        return new_inputs, labels"
      ],
      "metadata": {
        "id": "QCf0bu7Zl06V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as tf\n",
        "from torchvision import datasets\n",
        "\n",
        "\n",
        "my_transform = tf.Compose([tf.Resize((32,32)),tf.ToTensor()])\n",
        "path2data = '/content1/MyDrive/MNistData'\n",
        "\n",
        "train_data = datasets.MNIST(root = path2data,\n",
        "                            train = True,\n",
        "                            download = True,\n",
        "                            transform = my_transform)\n",
        "val_data = datasets.MNIST(root = path2data,\n",
        "                          train = False,\n",
        "                          download = True,\n",
        "                          transform = my_transform)\n",
        "\n",
        "train_dl = DataLoader(train_data, \n",
        "                      batch_size=32, \n",
        "                      shuffle = True)\n",
        "val_dl = DataLoader(val_data, \n",
        "                    batch_size=32)"
      ],
      "metadata": {
        "id": "8926AIeCrrRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import interact\n",
        "@interact(idx=(0,train_data.data.shape[0]))\n",
        "def showImage(idx):\n",
        "    plt.imshow(train_data.data[idx].numpy(), cmap=\"gray\")\n",
        "    plt.title(\"Label : {}\".format(train_data.targets[idx]))\n",
        "    plt.grid(False)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "fZTSXXz7y5JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# training data를 추출합니다.\n",
        "x_train, y_train = train_data.data, train_data.targets\n",
        "\n",
        "# val data를 추출합니다.\n",
        "x_val, y_val = val_data.data, val_data.targets\n",
        "\n",
        "# 차원을 추가하여 B*C*H*W 가 되도록 합니다.\n",
        "if len(x_train.shape) == 3:\n",
        "    x_train = x_train.unsqueeze(1)\n",
        "\n",
        "if len(x_val.shape) == 3:\n",
        "    x_val = x_val.unsqueeze(1)\n",
        "\n",
        "# tensor를 image로 변경하는 함수를 정의합니다.\n",
        "def show(img):\n",
        "    # tensor를 numpy array로 변경합니다.\n",
        "    npimg = img.numpy()\n",
        "    # C*H*W를 H*W*C로 변경합니다.\n",
        "    npimg_tr = npimg.transpose((1,2,0))\n",
        "    plt.imshow(npimg_tr, interpolation='nearest')\n",
        "\n",
        "# images grid를 생성하고 출력합니다.\n",
        "# 총 40개 이미지, 행당 8개 이미지를 출력합니다.\n",
        "x_grid = utils.make_grid(x_train[:40], nrow=8, padding=2)\n",
        "\n",
        "show(x_grid)"
      ],
      "metadata": {
        "id": "yBPyA-Xvwfg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LeNet_5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1,6, kernel_size=5, stride=1, padding=0)\n",
        "        self.bn1 = nn.BatchNorm2d(6)\n",
        "        self.conv2 = nn.Conv2d(6,16, kernel_size=5, stride=1, padding=0)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1, padding=0)\n",
        "        self.bn3 = nn.BatchNorm2d(120)\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.fc2 = nn.Linear(84,10)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        \n",
        "        nn.init.xavier_normal_(self.conv1.weight)\n",
        "        nn.init.xavier_normal_(self.conv2.weight)\n",
        "        nn.init.xavier_normal_(self.conv3.weight)\n",
        "        nn.init.xavier_normal_(self.fc1.weight)\n",
        "        nn.init.xavier_normal_(self.fc2.weight)\n",
        "\n",
        "\n",
        "    # dropout의 경우 model.train()시 사용되지만, model.eval()시 사용되지 않는다.\n",
        "    def forward(self, x):\n",
        "        x = F.tanh(self.bn1(self.conv1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.avg_pool2d(x, 2, 2)\n",
        "        x = F.tanh(self.bn2(self.conv2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = F.avg_pool2d(x, 2, 2)\n",
        "        x = F.tanh(self.bn3(self.conv3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(-1, 120)\n",
        "        x = F.tanh(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "model = LeNet_5()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "c6U37SHWxsNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 CUDA device로 전달한다. \n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "print(next(model.parameters()).device)"
      ],
      "metadata": {
        "id": "luQ_zMQU4U1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model, input_size=(1,32,32))"
      ],
      "metadata": {
        "id": "Cvju9Ix44eWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduction의 경우 값들의 연산을 나타낸다. \n",
        "# mean, sum, 등등 가능하다. \n",
        "loss_func = nn.CrossEntropyLoss(reduction='sum')"
      ],
      "metadata": {
        "id": "KiIupzwv4svm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "# optimizer의 param_groups는 parameter들을 dictionary 형태로 가지고 있다. \n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "# learning rate이 Cosine함수를 따라 변한다. -> learning rate이 최대에서 최소까지 계속 반복\n",
        "# T_max :  최대 iter횟수\n",
        "# eta_min : learning rate의 최솟값\n",
        "#from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "#lr_scheduler = CosineAnnealingLR(optimizer, T_max=2, eta_min=1e-05)"
      ],
      "metadata": {
        "id": "T1Pgf8Fo6ILs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "def metrics_batch(output, target):\n",
        "    pred = output.argmax(dim=1, keepdim=True)\n",
        "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
        "    return corrects\n",
        "\n",
        "def loss_batch(loss_func, output, target, opt=None):\n",
        "    loss = loss_func(output, target)\n",
        "    metric_b = metrics_batch(output, target)\n",
        "    if opt is not None:\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    return loss.item(), metric_b\n",
        "\n",
        "def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n",
        "    running_loss = 0.0\n",
        "    running_metric = 0.0\n",
        "    len_data = len(dataset_dl.dataset)\n",
        "\n",
        "    for xb, yb in dataset_dl:\n",
        "        xb = xb.type(torch.float).to(device)\n",
        "        yb = yb.to(device)\n",
        "        output = model(xb)\n",
        "        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)\n",
        "        running_loss += loss_b\n",
        "\n",
        "        if metric_b is not None:\n",
        "            running_metric += metric_b\n",
        "        \n",
        "        if sanity_check is True:\n",
        "            break\n",
        "    \n",
        "    loss = running_loss / float(len_data)\n",
        "    metric = running_metric / float(len_data)\n",
        "    return loss, metric\n",
        "\n",
        "def train_val(model, params):\n",
        "    num_epochs = params['num_epochs']\n",
        "    loss_func = params['loss_func']\n",
        "    opt = params['optimizer']\n",
        "    train_dl = params['train_dl']\n",
        "    val_dl = params['val_dl']\n",
        "    sanity_check = params['sanity_check']\n",
        "    path2weights = params['path2weights']\n",
        "\n",
        "    loss_history = {\n",
        "        'train' : [],\n",
        "        'val' : []\n",
        "    }\n",
        "\n",
        "    metric_history = {\n",
        "        'train' : [],\n",
        "        'val' : []\n",
        "    }\n",
        "\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        current_lr = get_lr(opt)\n",
        "        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs-1, current_lr))\n",
        "        model.train()\n",
        "        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n",
        "\n",
        "        loss_history['train'].append(train_loss)\n",
        "        metric_history['train'].append(train_metric)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n",
        "            loss_history['val'].append(val_loss)\n",
        "            metric_history['val'].append(val_metric)\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(model.state_dict(), path2weights)\n",
        "            print('Copied best model weights')\n",
        "\n",
        "        #lr_scheduler.step()\n",
        "\n",
        "        print('train loss: %.6f, dev loss: %.6f, accuracy: %.2f' %(train_loss, val_loss, 100*val_metric))\n",
        "        print('-'*10)\n",
        "        \n",
        "\n",
        "    # best model을 반환합니다.\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, loss_history, metric_history"
      ],
      "metadata": {
        "id": "0FkewhwM8I6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_train={'num_epochs':30, 'loss_func':loss_func, 'optimizer':optimizer, 'train_dl':train_dl, 'val_dl': val_dl, 'sanity_check':False, 'path2weights':'/content1/MyDrive/MNistData/best_model_dropout_xavierInitialization.pt'}\n",
        "model, loss_hist, metric_hist = train_val(model, params_train)"
      ],
      "metadata": {
        "id": "3Sju230lBBEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = params_train[\"num_epochs\"]\n",
        "\n",
        "plt.title(\"Train-Val Loss\")\n",
        "plt.plot(range(1, num_epochs+1), loss_hist[\"train\"], label=\"train\")\n",
        "plt.plot(range(1, num_epochs+1), loss_hist[\"val\"], label=\"val\")\n",
        "plt.ylabel(\"Label\")\n",
        "plt.xlabel(\"Training Epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g4IQBOWsB5hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Train-Val Accuracy\")\n",
        "plt.plot(range(1,num_epochs+1),metric_hist[\"train\"],label=\"train\")\n",
        "plt.plot(range(1,num_epochs+1),metric_hist[\"val\"],label=\"val\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Training Epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kVcBlWreJB-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ao5T3XTtJHd7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}