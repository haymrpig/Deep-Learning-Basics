# 확률론 기초

- **목차**
  
  1. [확률의 단순한 정의와 이항계수](#1-확률의-단순한-정의와-이항계수)
  2. [확률의 성질](#2-확률의-성질)
     - 확률의 성질 2가지
     - 확률의 속성들
     - Birthday problem
  3. [조건부 확률](#3-조건부-확률)
     - 독립
     - 조건부 확률
     - 전체 확률의 법칙
     - 베이즈 정리
     - Monty Hall problem ( 수형도, 전체 확률의 법칙 풀이)
     - 심슨의 역설 (Simpson's Paradox)
  4. [확률변수](#4-확률변수-random-variable)
     - 도박꾼의 파산 (Gambler's ruin)
     - 확률질량함수 (PMF)
     - 누적분포함수 (CDF)
  5. [PMF와 CDF](#5-pmf와cdf)
     - 기댓값 (Means, Expected values)
  6. [이산분포](#6-이산분포)
     - 베르누이 분포 (Bernoulli)
     - 이항분포 (Binomial)
     - 음이항분포 (Negative Binomial)
     - 초기하분포 (Hypergeometric)
     - 기하분포 (Geometric)
  7. [연속분포](#7-연속분포)
     - 균등분포 (Uniform)

# 1. 확률의 단순한 정의와 이항계수

- **용어 정리**

  - **표본 공간 (S, sample space)** : 어떤 시행에서 일어날 수 있는 모든 결과들의 모임

  - **사건 (event)** : 표본공간 (sample space)의 부분집합으로 어떤 조건을 만족하는 특정한 표본점들의 집합

    

- **확률의 단순한 정의 (Naive definition)**

  P(A)로 표기하며 ( 가능한 A의 결과의 수 ) / (가능한 모든 경우의 수)를 의미한다.

  - # 확률의 분모는 유한한 값을 가져야 한다.

  - 단순한 정의로 해결할 수 있는 문제는 확률이 동일한 경우에만 해당한다.

    ex) 해왕성에 생물이 살 확률은? -> 단순한 정의에 의하면 산다/안산다 이므로 50%이다.

    50%의 확률은 말도 안되는 값임을 알 수 있다. 

    ex2) 해왕성에 지능을 가진 생물이 살 확률은 ? -> 마찬가지로 50%이다. 직관적으로 생각해봐도 생물이 살 확률보다 지능을 가진 생물이 살 확률이 훨씬 낮아야하지만 그렇지 않기에 문제가 된다.



- **이항계수 (Binomial Coeffecient)**

  자연수 n 및 정수 k가 주어졌을 때 **nCk** 또는 **C(n,k)** 로 표기한다.

  수식으로 나타내면 
  ![image](https://user-images.githubusercontent.com/71866756/147647425-56ac7a9c-61ec-4232-8bcd-45319eee6e7a.png)

  - 이항계수의 3가지 성질

    - C(n, k) = C(n, n-k) (이항계수의 정의를 이용해 증명 가능)

    - nC(n-1, k-1) = kC(n, k)

      증명) n명의 사람들 중 k명을 뽑고, 그 중 한명을 대표로 뽑는 경우를 생각할 수 있다. 

      ​		  (좌변) 대표를 먼저 뽑는 경우 : n명 중 1명 (n), n-1명 중 k-1명 ( C(n-1, k-1) )

      ​          (우변) k명을 먼저 뽑는 경우 : n명 중 k명 ( C(n, k) ), k명 중 1명 (k) 
      
    - 방데르몽드 항등식
    
    ![image](https://user-images.githubusercontent.com/71866756/147647465-25b2cc70-eaad-4c8a-ada3-82082c1039da.png)


  **!! 주의할 점 !!**

  10명 중 4명, 6명으로 두팀을 만든다고 하였을 때의 확률은 10C6=10C4이다.

  하지만, 10명 중 5명, 5명으로 두팀을 만든다고 하였을 때의 확률은 10C5 / 2가 된다.

  ( 두 팀을 완벽히 구분이 가능하다면 2로 나누지 않아도 되지만, 구분이 불가능할 경우 중복으로 counting이 되기 때문에 2로 나눠줘야 한다. )

  

  |                        | 순서상관있는 경우 | 순서상관없는 경우 |
  | ---------------------- | ----------------- | ----------------- |
  | **복원하는 경우**      | n^k               | n+k-1Ck           |
  | **복원하지 않는 경우** | nPk               | nCk               |

  - 순서상관없고, 복원하는 경우의 n+k-1Ck 증명

    ![image](https://user-images.githubusercontent.com/71866756/147635035-0e217ff2-4ec0-45f3-829b-c29895dcfa20.png)




# 2. 확률의 성질

- **확률의 성질 2가지**

  - 표본 공간 S, S의 사건 A를 입력으로 받았을 때, 0~1 사이의 값을 출력으로 갖는 함수 P가 있다고 했을 때, 만족하는 성질 2가지
  
  ![image](https://user-images.githubusercontent.com/71866756/147647362-3e1f6d3c-67e4-4fbe-9e80-e8181c5abfc8.png)
  
  
  
- **확률의 성질로 증명할 수 있는 속성들**

  - 공집합의 확률  
    ![image](https://user-images.githubusercontent.com/71866756/147647495-434be893-6920-460b-aa3b-1e0272e493d3.png)
  
  
  
- **여집합의 확률**  
    ![image](https://user-images.githubusercontent.com/71866756/147647507-c6d75e96-03d6-4c86-969a-5d83c221a881.png)
    
  - 포함 배제의 원리  
    ![image](https://user-images.githubusercontent.com/71866756/147647528-91d6125e-406a-40a7-a9a8-e1c798f2d966.png)
    - 증명  
    ![image](https://user-images.githubusercontent.com/71866756/147647546-b2a93ebb-ebca-450b-aa4c-362a0fe91f2d.png)
  
  
  
- **Birthday Problem**

  k명의 사람들 중 적어도 2명의 생일이 같은 확률이 50%가 넘는 k의 값은?

  ( 365일 중 태어날 확률은 모두 equally likely라고 가정하면 단순한 정의를 이용할 수 있다. )

  - k > 365일 경우

    **비둘기집의 원리**로 k>365보다 크면 적어도 한 쌍은 생일이 같을 수 밖에 없다. 

  - k <= 365일 경우

    : 직관적으로 생각해보면 365일 중 생일이 같은 확률은 크다고 생각되지 않는다. 하지만 이 문제의 정답은 23명이다. 

    P(생일이 모두 다를 경우) = ( 365x364x...x(365-k+1) ) / ( 365^k ) 으로 나타낼 수 있다. 

    P( 생일이 적어도 한명 같은 경우 ) = 1 - P(생일이 모두 다를 경우) 가 된다. 

    따라서 **k=23일 경우 확률은 약 50.7%**, **k=100일 경우 약 99.999%**가 나온다.

    -> 이 문제를 조금 다른 시각으로 보면 k=23일 때, 23C2로 생일이 같은 경우의 조합의 수를 생각해볼 수 있다. 23C2 = 253으로 상당히 많은 경우의 수가 나올 수 있다. 이러한 시각으로 봤을 때, 두명의 생일이 같을 확률은 꽤 크다고 볼 수 있다.

     ```c++
     // 실제 코드로 작성하여 난수 생성하여 실험해본 결과 50%에 육박하게 나오는 것을 확인할 수 있었다. 
     #include <iostream>
     #include <cstdlib>
     #include <map>
     #include <ctime>
     using namespace std;
     
     int main() {
     	int cnt = 0;
     	srand((unsigned int)time(NULL));
     	for (int k = 0; k < 10; k++) {
     		for (int j = 0; j < 1000; j++) {
     			map<int, int> m;
     			for (int i = 0; i < 23; i++) {
     				int temp = rand() % 365 + 1;
     				if (m[temp] != 0) {
     					m[temp] += 1;
     					cnt++;
     					break;
     				}
     				else m[temp] = 1;
     			}
     		}
     		cout << k+1 << "번째 확률 : " << (double)cnt / (double)1000 << endl;
     		cnt = 0;
     	}
     }
     ```

     ![image](https://user-images.githubusercontent.com/71866756/147647720-edcfb2e0-c2b3-4cab-93c0-00d91fe0fb5a.png)

# 3. 조건부 확률

- **독립**

  사건 A,B는 P(A,B)=P(A)P(B)일 경우, 독립이라고 한다. 

  

- **조건부 확률**

  사건 B가 발생했을 때 A가 발생할 확률로 만약 A와 B가 독립이 아닌 경우 매우 유용하다. 

  **!!! 중요 !!!** 

  여기서 P(B)로 나눠주는 이유는 B를 이미 발생한 사건으로 보고 A가 발생할 확률을 구하는 것이기 때문에 전체 표본 공간이었던 S가 B로 축소되고 이를 정규화하기 위해 P(B)로 나누어주는 것이다.   
  ![image](https://user-images.githubusercontent.com/71866756/148025973-9d076724-c5b6-43a0-a0d6-016fc3a20441.png)

   

  A와 B가 독립인 경우는 아래 수식으로 나타낼 수 있다.  
  ![image](https://user-images.githubusercontent.com/71866756/148026015-d4f72e20-3aad-44a9-bf8e-c023a9ca0c8f.png)

  - 정리

    ![image](https://user-images.githubusercontent.com/71866756/148026040-7f09ce2d-b65e-4b3f-bde4-5023e196827e.png)
    
    

- **전체 확률의 법칙 (law of total probability)**

  어려운 문제를 간단하게 쪼개어 해결하는 방법 중에 하나로 생각할 수 있다. 

  **조건부 확률**로부터 **조건이 붙지 않은 확률**을 계산할 때 사용할 수 있다.  

  (즉, P(B)를 B와 A의 조건부 확률로부터 계산 가능)

  ![image](https://user-images.githubusercontent.com/71866756/148051555-6f0b4b21-10a2-470b-a83a-8ad32a46b559.png)

- **베이즈 정리**  
  ![image](https://user-images.githubusercontent.com/71866756/148051588-34647411-7177-4a52-acd0-af68feac22e7.png)

  - 사전 확률 (P(A), prior) : 주어진 확률

  - 사후 확률 (P(A|B), posterior) : 구하는 확률

  - 조건부 독립 (conditional independence) : 사건 A,B가 사건 C가 주어졌을 경우 독립인 상황
    ![image](https://user-images.githubusercontent.com/71866756/148051622-73dda008-b933-4d76-b000-96498346ae99.png)

  **!!! 주의 !!!**

   조건부 독립과 독립은 별개이다. 독립이라고 해서 무조건 조건부 독립이 아니고, 조건부 독립이라고 해서 무조건 독립인 것도 아니다. 

  

  **EX)** 질병이 걸릴 확률은 1%이고 test가 정확한 확률은 95%일 때, 실제 질병에 걸렸을 확률은?

  P(D) : 질병에 걸렸을 확률

  P(T) : 테스트 결과 질병에 걸렸을 확률

  P(T|D) = 0.95  
  ![image](https://user-images.githubusercontent.com/71866756/148051650-2cf0cb58-ae61-4864-bce4-cf7044ad1e56.png)  
  => 실제 질병에 걸렸을 확률은 0.16으로 매우 낮은 값을 보인다. 

  => 직관적으로는 test 정확도가 높기 때문에 test결과 질병에 걸렸다고 나왔을 경우, 실제 질병에 걸렸을 확률은 더욱 높을 것으로 예상하지만 **직관과는 정반대의 결과**를 보인다.

  => 이는 test 정확도에 치중하여 생각하기 때문이다. 실제로 **병의 발생확률도 고려**해야 하기 때문에 이와 같은 결과를 보이는 것이다. 



- **Monty Hall 문제**

  - 수형도를 이용하여 풀이한 경우

    ![image](https://user-images.githubusercontent.com/71866756/148051687-286a83c1-39e1-4aa2-8f19-f1a5d59e20da.png)  

    

  - 전체 확률의 법칙을 이용한 경우

    S : 차를 뽑는 경우 (가정은 항상 선택을 바꾼다.)

    Dj : j번 문 뒤에 차가 있는 경우

    !! 처음에 1번 문을 선택한다고 가정하자 !!
    ![image](https://user-images.githubusercontent.com/71866756/148051761-b229105b-40bc-4eb9-9a71-21c9ccfc95bb.png)  



- **심슨의 역설 (Simpson's Paradox)**

  개별적인 항목에서 항상 뛰어난 결과여도 항목들의 합에서는 반대의 결과가 나타난다는 paradox

  | Person1  | 심장 수술 | 붕대풀기 | Person2  | 심장 수술 | 붕대풀기 |
  | -------- | --------- | -------- | -------- | --------- | -------- |
  | **성공** | 70        | 10       | **성공** | 2         | 81       |
  | **실패** | 20        | 0        | **실패** | 8         | 9        |

  - 위 케이스를 살펴보면 Person1의 각각의 항목의 성공률이 Person2보다 높은 것을 알 수 있다.

    ( 조건부 ) 

  - 하지만 전체 항목의 합은 Person1은 80%, Person2는 83%로 Person2가 높은 것을 알 수 있다. 

    ( 비조건부 )

  - A : surgery succeed, B : Person2이 수술, C : 심장 수술

    수식으로 표현하자면 아래처럼 될 것이다.   
    ![image](https://user-images.githubusercontent.com/71866756/148051795-8e30d406-617d-4da7-91ef-a5fd30e5b492.png)  
  
  - 이것이 심슨의 역설이다. 이것이 잘못된 이유는 아래 식으로 볼 수 있다.   
    ![image](https://user-images.githubusercontent.com/71866756/148051824-ed4d6db5-e32c-4e5f-8249-fbc826ecfbb0.png)  
    여기서 P(C|B)와 P(C^c|B)는 가중치라고 생각할 수 있는데, Person2가 해당 수술을 얼마나 진행했는지 나타내는 확률이다. 이 값이 들어간다면 심슨의 역설은 성립하지 않게 된다. 이것이 심슨의 역설이 존재할 수 있게 만드는 이유이다.  

  **!!! 따라서 교란 요인 (위 예에서는 C) 을 고려하는 것이 매우 중요하다 !!!**



# 4. 확률변수 (Random Variable)

- **확률 변수란**

   확률 변수는 표본 공간에서 실수로 가는 함수이다. 간단하게 말하자면 임의의 확률 시행의 수치적인 "요약"이라고 할 수 있다. 꼭 전체 표본공간의 요약일 필요는 없다. 확률변수를 사용하는 이유는 다루기 어려운 표본 공간에서 다루기 쉬운 실수로 대응할 수 있기 때문이다. 

- **도박꾼의 파산 (Gambler's Ruin)**

  A와 B 두명의 도박꾼이 매 라운드 $1씩 걸고 도박을 한다. 이긴 사람은 상대방의 $1을 가져가고, 둘 중 한명이 가지고 온 돈이 바닥날 때까지 이 과정을 반복한다. 

  p = P(A가 어떤 라운드를 이긴다.)

  q = 1-p

  A는 i달러, B는 N-i 달러를 가지고 게임을 한다고 할 때,    
  ![image](https://user-images.githubusercontent.com/71866756/148051852-0330f16e-ab0f-44bc-9135-7b50343c7247.png)  
    _위 수식을 계차방정식이라고 한다. 미분 방정식의 discrete형태를 의미한다._ 

  - 계차방정식의 풀이  

    ![image](https://user-images.githubusercontent.com/71866756/148051886-2241ef94-aa29-4d6e-a2ca-6364109f8a43.png)



- **확률질량함수 (PMF, probability mass function)**

   사건의 발생확률을 구할 수 있으며, 이산확률일 경우에 사용된다.   
    ![image](https://user-images.githubusercontent.com/71866756/148190225-2c1348be-c147-4fdc-9fec-72f70a5b9b2f.png)  


- **누적분포함수 (CDF, cumulative distribution function)**

  확률질량함수는 이산확률일 경우에만 사용할 수 있지만, 누적분포함수는 연속확률일 경우에도 사용할 수 있기 때문에 좀 더 일반적이라고 할 수 있다.   
    ![image](https://user-images.githubusercontent.com/71866756/148190269-1b896bb5-5593-4041-8dd3-2f8a06ee7d28.png)  
  
  

# 5. PMF와CDF

- **이산확률분포에서의 CDF 그래프**  

  ![image](https://user-images.githubusercontent.com/71866756/148191589-2be24773-70eb-475d-8531-65669b49565b.png)  

  - !! CDF의 특징 !!

    - 증가함수이다. (증가하거나 같다, 감소할 수는 없다)

    - 우연속이다. 

      예를 들어, 2.x에서 2로 갈때 값이 바뀌지 않지만, 1.x에서 2로 갈 때는 값이 바뀐다.  

    - x가 음의 무한대로 갈 때, F(x)는 0으로 간다. 

      x가 양의 무한대로 갈 때, F(x)는 1로 간다.

      

- **CDF에서의 독립**  
  ![image](https://user-images.githubusercontent.com/71866756/148191653-faa37766-a4f4-4edf-9379-68fff8f0207c.png)  
  위 식이 성립한다. 

- **기댓값 (Means, Expected values)**

  - 기댓값을 구하는 두가지 방법

    - 산수

    - 가중평균 (곱해지는 가중치가 다를 경우)

      비가중평균 (곱해지는 가중치가 모두 같을 경우)

    **EX)** 

    1,1,1,1,1,5,5,8의 합을 산수로 구하면 => (1+1+...+8)/8

    1,1,1,1,1,5,5,8의 합을 가중평균으로 구하면 => (5/8) * 1 + (2/8) * 5 + (1/8) * 8

    

  - 이산확률분포의 기댓값 ( E(X) )  
    ![image](https://user-images.githubusercontent.com/71866756/148191705-643c55ec-3553-4c65-b90b-6c81945d5af5.png)  

  - 선형성 (Linearity)  
    ![image](https://user-images.githubusercontent.com/71866756/148191809-c3f0a875-4054-435e-a001-de0352859119.png)  
    1번 증명)  

    ![image](https://user-images.githubusercontent.com/71866756/148191862-08cac672-d07c-470d-832e-75121fe3ae5e.png)  
    => grouping의 경우 가중평균으로 기댓값을 구하는 것이고, ungrouped의 경우 산수로 구하는 것이라고 생각할 수 있다. 

    => ungrouped의 경우 (0+0+0+0+1+1+2+2+2+3)*(1/10)이라고 표현할 수 있다. 

    따라서 아래 식처럼 증명할 수 있다. (T=X+Y)  

    ![image](https://user-images.githubusercontent.com/71866756/148191935-5e30aa67-e364-47e1-8b46-a138ef44e034.png)  

- **분산 (Variance)**

  분산은 분포의 퍼짐 정도를 알려준다. 분포의 값들이 평균으로부터 떨어진 정도를 나타낸다.   
  ![image](https://user-images.githubusercontent.com/71866756/148350746-2a9a14a5-14f5-4d6f-a05e-98da62c76503.png)  

  제곱을 하는 이유는  
  ![image](https://user-images.githubusercontent.com/71866756/148350783-7ccb1919-2600-46c1-a987-62559d79704a.png)  
  위 식처럼 0으로 의미없는 값이 되기 때문이다. 

  하지만 제곱을 함으로써 기존 단위와 달라지므로 단위를 맞추기 위해서 표준편차가 등장하였다. 

  

-  **표준편차 (Standard Deviation)**  
  ![image](https://user-images.githubusercontent.com/71866756/148350829-c2bbf10b-47be-4a4a-8f55-f5f66d186b69.png)  
  

# 6. 이산분포


- **완전 독립성**  
    ![image](https://user-images.githubusercontent.com/71866756/148350871-32aeda5a-c1ab-4b50-9916-ffcfa8feb14b.png)  
    완전 독립과 쌍으로 독립 (pairwise independent)은 다르다.

    **EX)** X1,X2~Bern(1/2)인 동전 던지기 게임이라고 하자. X1과 X2가 같은 면일 경우 이긴다고 하였을 때, 새로운 확률변수 X3는 아래 식과 같다.  
    ![image](https://user-images.githubusercontent.com/71866756/148350905-96858b18-93f3-44e7-8f29-2828142d308d.png)  
     이 경우, (X1,X2) (X1,X3) (X2,X3)는 pairwise independent하지만 X1과 X2를 알면 X3를 알 수 있기 때문에 X1,X2,X3는 완전 독립이라고 할 수 없다.

    

- **이항정리 (Binomial theorem)**  
    ![image](https://user-images.githubusercontent.com/71866756/148190324-06a34d2d-6cc1-4d4e-a575-a635749edbf0.png)  


- **베르누이 분포 ( Bern(p), Bernoulli )**

  X가 0(실패), 1(성공) 두가지의 값만 가질 수 있으며, 

  **P(x=1)=p, P(x=0)=1-p**일 때, X는 _Bernoulli(p)_분포를 따른다고 한다.  

  시행전의 x는 무엇인지 모르지만, 시행 후에는 0 또는 1이 된다. 



- **이항분포 ( Bin(n,p), Binomial )**

  n번의 **독립**적인 베르누이(p) 시행에서 성공 횟수의 분포는 _Bin(n,p)_ 를 따른다고 한다. 

  - 확률질량함수 (PMF, probability mass function)  
    
    사건의 발생확률을 구할 수 있으며, **이산확률변수**에만 해당한다.  
    ![image](https://user-images.githubusercontent.com/71866756/148051921-76250018-702f-4f46-bb5f-f08cb1570f91.png)
    
    => 이항분포는 **이항정리**와 같은 형태로 PMF의 조건이 성립한다는 것을 알 수 있다.   
      ![image](https://user-images.githubusercontent.com/71866756/148190536-b3444aef-3735-43bd-b839-b50ec631a964.png)  
  
  
  
  - 지시확률변수 (indicator random variables)
  
    아래 수식처럼 발생하는 경우의 수를 모두 count하는 방법이다. 
  
    **iid** : independent, identically distributed ( X는 모두 같은 분포를 가지며 서로 독립이라는 의미)  
      ![image](https://user-images.githubusercontent.com/71866756/148190580-2f29218c-30cb-44c7-a550-7aa51bc15b82.png)  
    
  - _X ~ Bin(n,p), Y ~ Bin(m,p)_ 이면서 X와 Y가 **독립**일 경우, _X+Y~Bin(n+m,p)_ 이다.   
      ![image](https://user-images.githubusercontent.com/71866756/148190614-e7dd660a-38f9-4530-b7fd-ca2bf6fc9762.png)  
    
      ![image](https://user-images.githubusercontent.com/71866756/148190640-1d59ef9f-c6ae-468a-aba9-f1f02b31eed9.png)  
  
- **음이항분포 (NegBin(r,p), Negative Binomial)**

  여러 번의 Bern(p) 독립시행 중에서 r번째 성공까지의 실패 횟수를 나타낸다. 

  - PMF

    성공 확률 : p, 실패 확률 : q, 성공 횟수 : r, 실패 횟수 : n일 때, 마지막은 항상 성공이여야 하므로, 마지막 이전 시행들의 조합을 구하면 된다.    
     ![image](https://user-images.githubusercontent.com/71866756/148190698-014b55e5-96a9-4ecd-a6dc-0d40d3312c37.png)  
    

- **초기하분포 (Hypergeometric)**

  **EX)** 하얀 돌과 검정돌이 각각 w개, b개 있다. 이중에서 하얀돌을 뽑는 확률 = X라고 할 때, 하얀돌을 k개 뽑는 확률을 구하라.    
    ![image](https://user-images.githubusercontent.com/71866756/148190754-74186eb5-7531-4bf4-9030-9ccff66eb5e9.png)  
  => 이러한 분포를 초기하분포라고 한다. (비복원추출, 이 경우 독립적이지 않기 때문에 binomial일 수가 없다. )  
    ![image](https://user-images.githubusercontent.com/71866756/148190792-0642f3b3-ca6f-43aa-bd1d-f99594fa4683.png)  


- **기하분포 (Geom(p),Geometric)**

  독립적인 베르누이 시행에서 첫 성공 전의 실패 횟수를 count.

  - PMF  
      ![image](https://user-images.githubusercontent.com/71866756/148190838-d0b09e85-f246-47bc-b632-4362c6229c8b.png)
  
    

- **포아송분포 (Pois(lambda), Poisson distribution)**

   실제 이산형 데이터의 모델로 가장 많이 사용하는 분포이다. 포아송 분포는 주로 수를 세는 경우에 사용된다. 아주 많은 시도에서 작은 수를 세는데 사용된다. 

   포아송은 이항분포에서 n이 무한대, p가 0으로 근사할 때 근사될 수 있다. 이 근사를 사용하는 이유는 n이 너무 크면 계산이 복잡하기 때문에 포아송의 간단한 계산으로 대체할 수 있기 때문이다. 

   예를 들어, 한시간 동안 받는 이메일의 갯수, 초코쿠키 안의 초코칩 갯수, 특정 지역에서의 1년간 지진 발생 수를 들 수 있다. 모두 정확히 포아송 분포가 아닐 수도 있지만, 근사할 것이다. 이런 경우 포아송 분포를 사용할 수 있다. 

  - PMF  

  ![image](https://user-images.githubusercontent.com/71866756/148351004-1083ee81-da1a-46d3-8cd6-69945bf9f80c.png)  

  - Poisson paradigm

    포아송과 이항분포는 유사하지만, 포아송의 경우, 무조건 독립일 필요없이 약한 의존성을 가져도 되며, 발생확률이 모두 같지 않아도 된다. 그러므로 포아송이 좀 더 일반적인 경우가 되는 것이다.   
    ![image](https://user-images.githubusercontent.com/71866756/148351041-f739f227-9b7f-4af5-aa62-111cffd240a5.png)  

  - 이항분포와 포아송의 관계  
    ![image](https://user-images.githubusercontent.com/71866756/148351118-e6f81cc0-b246-4b2f-a1cd-b316be3c9545.png)  

  **EX)** 사각형 판 위에 빗방울이 떨어지는 확률 -> 포아송 분포

  **EX1)** n명의 사람들 중 3명의 생일이 같을 확률의 근사값을 구하라. -> 포아송 분포 근사

  

# 7. 연속분포

- **확률밀도함수 (PDF, Probability Density Function)**  
  ![image](https://user-images.githubusercontent.com/71866756/148351168-a3ce8fb6-4b5c-41f7-8cc0-76193755e4c6.png)  
  

  위의 식을 만족하는 확률 변수 X는 PDF f(x)를 가진다.  

  PDF의 경우 특정 값을 갖는 경우(a=b)일 때의 확률은 0이다. 

  - PDF 조건 2가지  
    ![image](https://user-images.githubusercontent.com/71866756/148351200-598eccea-dfae-40a6-8a26-deebf51b4255.png)  

  - CDF from PDF  
    ![image](https://user-images.githubusercontent.com/71866756/148351247-46e78bb8-fd1a-47ea-8ab5-4b509e2ed29d.png)  

  - PDF from CDF  
    ![image](https://user-images.githubusercontent.com/71866756/148351286-f18b4894-cbc4-4a08-88b4-0a00f4e40f85.png)  

  - PDF & CDF  
    ![image](https://user-images.githubusercontent.com/71866756/148351314-cdac2127-227a-40b0-9bb6-219c58b405a3.png)  

- **PDF의 기댓값**  
  ![image](https://user-images.githubusercontent.com/71866756/148351345-7d51c8c5-2f34-49a7-a56e-568a789153d5.png)  
  

- **완전 독립성**  
  ![image](https://user-images.githubusercontent.com/71866756/148351381-6503cd4b-5b07-401a-9222-ba23f13e62de.png)  
  

- **균등분포 (Unif(a,b), Uniform distribution)**

  - PDF  
    ![image](https://user-images.githubusercontent.com/71866756/148351406-ab2ad0f7-1410-4b38-a9a4-b1322e7875d8.png)  

  - CDF  
    ![image](https://user-images.githubusercontent.com/71866756/148351429-b4807a71-b9e8-4599-8068-f92a821381e4.png)  

  - 균등분포의 일반성 (Uniform is universal)

    균등분포는 **어떠한 분포**로도 **전환 가능**하다.   
    ![image](https://user-images.githubusercontent.com/71866756/148351466-b240656e-df81-4e29-ba7d-bf870e6c62df.png)  

  - 균등분포의 대칭성  
    ![image](https://user-images.githubusercontent.com/71866756/148351504-ba0ee35a-1244-4a52-ae72-dbb880d66b43.png)  

  - 균등분포의 선형성  
    ![image](https://user-images.githubusercontent.com/71866756/148351531-b147b38b-2325-4a69-aa01-6a778eb9881f.png)  

- **정규분포/가우시안 (Normal distribution)**

  - 중심극한정리 (Central Limit Theorem)

    여러개의 독립적이고 동일한 확률변수를 더했을 때, 그 합의 분포가 정규분포를 따른다는 정리 

    

  - 표준정규분포 ( N(0,1) )

    - 표준 표기법  

    ![image](https://user-images.githubusercontent.com/71866756/148351568-468f8cbf-a51b-42a1-b045-66ec5c046321.png)  

    평균이 0, 분산이 1인 정규분포를 의미한다. 보통 x보다는 z라고 많이 쓴다.

    ( 대칭이기 때문에 평균이 0이다. 적분 시 기함수이기 때문에 0이 나온다. )  
    ![image](https://user-images.githubusercontent.com/71866756/148351607-1255c10a-521b-44c7-8daf-af9f7fe901d6.png)  

    - 정규화 상수 구하기

      정규화상수를 구하기 위해서 PDF의 합이 1인 것을 이용한다.  
      ![image](https://user-images.githubusercontent.com/71866756/148351647-294b44a2-99ac-4839-b99b-20c1c96de3cd.png)  
      하지만, 위의 적분은 부정적분으로는 절대 풀 수가 없는 식이다!!

      따라서, 정적분을 이용하는데, 이 때 야코비안에 대한 개념이 들어간다. ( 맨 아래 추가로 알면 좋은 공식에 야코비안에 대해 정리해놓았다. )  
      ![image](https://user-images.githubusercontent.com/71866756/148351682-03440ae8-eada-4b85-9cc1-36425d8f57de.png)  



| 이산 분포                  | 기댓값 | 분산 |
| -------------------------- | ------ | ---- |
| 베르누이 (Bernoulli)       | p      |      |
| 이항 (Binomial)            | np     |      |
| 음이항 (Negative Binomial) | (rq)/p |      |
| 기하 (Geometric)           | q/p    |      |
| 포아송 (Poisson)           | lambda |      |

1. **이항분포 기대값 증명**  

![image](https://user-images.githubusercontent.com/71866756/148192013-9488d083-e761-4004-8257-7343015ffd2c.png)  

2. **음이항분포 기대값 증명**  
   ![image](https://user-images.githubusercontent.com/71866756/148192063-846d0d30-4d90-4188-8e4c-eea0459a3a94.png)  
   
3. **기하분포 기대값 증명**  
   ![image](https://user-images.githubusercontent.com/71866756/148192112-846aa7fa-4cc6-4cae-b6a2-644d57a0c8ad.png)  

4. **포아송분포 기대값 증명**  
   ![image](https://user-images.githubusercontent.com/71866756/148351756-2dc1d790-b5cd-436a-bd7f-51a4eb1ebe02.png)  





| 연속분포       | 기댓값  | 분산        |
| -------------- | ------- | ----------- |
| 균등 (Uniform) | (a+b)/2 | (b-a)^2 /12 |

1. **균등 분포 기대값, 분산 증명**  
   ![image](https://user-images.githubusercontent.com/71866756/148351812-20b22e2a-7dcc-4526-990a-6c8c45232bbb.png)  






# 기타 유용한 공식들

- **테일러 급수**

  ![image](https://user-images.githubusercontent.com/71866756/148351877-f094bbb6-bc2b-4ebd-a0a4-2487f656d597.png)

- **포아송 증명에 쓰이는 공식**  
  ![image](https://user-images.githubusercontent.com/71866756/148351913-71dcbd00-9d41-49c4-8027-7bf404fce51e.png)  

- **야코비안 (Jacobian)**

  야코비안 행렬의 원소들은 모두 1차 미분 계수로 구성되어 있으며, 미소 변화에 관한 선형 변환이라는 것을 알 수 있다. 즉 야코비안이란 쉽게 말해서 미소 영역에서 **'비선형 변환'**을 **'선형 변환으로 근사'**시킨 것이다. 

  ![image](https://user-images.githubusercontent.com/71866756/148351945-0a26ecc7-daf7-454c-b990-849270352109.png)

  아래 그림에서 보면 알 수 있듯이, 비선형 변환을 국소적으로 관찰하면 선형 변환인 것을 알 수 있다. 야코비안은 이러한 특징을 나타낸다. 

  ![image](https://user-images.githubusercontent.com/71866756/148351986-826648bd-9f18-492e-ac3c-017466149d04.png)

  하나의 예제를 들어보겠다.   
  ![image](https://user-images.githubusercontent.com/71866756/148352037-fce2282b-550b-4cc5-80e9-8a88647eba1b.png)
  



